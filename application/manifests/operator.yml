{{ $kubeSystemNamespace := "kube-system" -}}
{{ $metricsPort := "6942" -}}
{{ $healthzPort := "9234" -}}
{{ $healthCheckSidecarPort := "9235" -}}
{{ $slsStatusReporterAppPort := "9236" -}}
{{ $slsStatusReporterMgmtPort := "9237" -}}
---
apiVersion: v1
kind: Namespace
metadata:
  name: {{ $kubeSystemNamespace }}
  labels:
    name: {{ $kubeSystemNamespace }}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: cilium-config
  namespace: {{ $kubeSystemNamespace }}
data:
  etcd-config: |-
    ---
    endpoints:
      - https://cilium-etcd-0.cilium-etcd.kube-system.svc.cluster.local:2379
      - https://cilium-etcd-1.cilium-etcd.kube-system.svc.cluster.local:2379
      - https://cilium-etcd-2.cilium-etcd.kube-system.svc.cluster.local:2379
    ca-file: '/mnt/secrets/certs/ca.pem'
    key-file: '/mnt/secrets/certs/key.pem'
    cert-file: '/mnt/secrets/certs/cert.pem'

  # If you want to run cilium in debug mode change this value to true
  debug: "false"

  # Needed because there will be a `/19` (or similar) route which conflicts with the `/32` route. For now, disable reservation of IPs which conflict with local routes.
  dont-allocate-conflicting-routes: "false"

  # If you want metrics enabled in all of your Cilium agents, set the port for
  # which the Cilium agents will have their metrics exposed.
  # This option deprecates the "prometheus-serve-addr" in the
  # "cilium-metrics-config" ConfigMap
  # NOTE that this will open the port on ALL nodes where Cilium pods are
  # scheduled.
  # prometheus-serve-addr: ":9090"

  # Enable IPv4 addressing. If enabled, all endpoints are allocated an IPv4
  # address.
  enable-ipv4: "true"

  # Enable IPv6 addressing. If enabled, all endpoints are allocated an IPv6
  # address.
  enable-ipv6: "false"

  # If a serious issue occurs during Cilium startup, this
  # invasive option may be set to true to remove all persistent
  # state. Endpoints will not be restored using knowledge from a
  # prior Cilium run, so they may receive new IP addresses upon
  # restart. This also triggers clean-cilium-bpf-state.
  clean-cilium-state: "false"
  # If you want to clean cilium BPF state, set this to true;
  # Removes all BPF maps from the filesystem. Upon restart,
  # endpoints are restored with the same IP addresses, however
  # any ongoing connections may be disrupted briefly.
  # Loadbalancing decisions will be reset, so any ongoing
  # connections via a service may be loadbalanced to a different
  # backend after restart.
  clean-cilium-bpf-state: "false"

  # Users who wish to specify their own custom CNI configuration file must set
  # custom-cni-conf to "true", otherwise Cilium may overwrite the configuration.
  custom-cni-conf: "true"

  # If you want cilium monitor to aggregate tracing for packets, set this level
  # to "low", "medium", or "maximum". The higher the level, the less packets
  # that will be seen in monitor output.
  monitor-aggregation: "none"

  # ct-global-max-entries-* specifies the maximum number of connections
  # supported across all endpoints, split by protocol: tcp or other. One pair
  # of maps uses these values for IPv4 connections, and another pair of maps
  # use these values for IPv6 connections.
  #
  # If these values are modified, then during the next Cilium startup the
  # tracking of ongoing connections may be disrupted. This may lead to brief
  # policy drops or a change in loadbalancing decisions for a connection.
  #
  # For users upgrading from Cilium 1.2 or earlier, to minimize disruption
  # during the upgrade process, comment out these options.
  bpf-ct-global-tcp-max: "524288"
  bpf-ct-global-any-max: "262144"

  # Pre-allocation of map entries allows per-packet latency to be reduced, at
  # the expense of up-front memory allocation for the entries in the maps. The
  # default value below will minimize memory usage in the default installation;
  # users who are sensitive to latency may consider setting this to "true".
  #
  # This option was introduced in Cilium 1.4. Cilium 1.3 and earlier ignore
  # this option and behave as though it is set to "true".
  #
  # If this value is modified, then during the next Cilium startup the restore
  # of existing endpoints and tracking of ongoing connections may be disrupted.
  # This may lead to policy drops or a change in loadbalancing decisions for a
  # connection for some time. Endpoints may need to be recreated to restore
  # connectivity.
  #
  # If this option is set to "false" during an upgrade from 1.3 or earlier to
  # 1.4 or later, then it may cause one-time disruptions during the upgrade.
  preallocate-bpf-maps: "false"

  # Regular expression matching compatible Istio sidecar istio-proxy
  # container image names
  sidecar-istio-proxy-image: "cilium/istio_proxy"

  # Encapsulation mode for communication between nodes
  # Possible values:
  #   - disabled
  #   - vxlan (default)
  #   - geneve
  tunnel: "disabled"

  masquerade: "false"

  # Name of the cluster. Only relevant when building a mesh of clusters.
  cluster-name: default

  # Unique ID of the cluster. Must be unique across all conneted clusters and
  # in the range of 1 and 255. Only relevant when building a mesh of clusters.
  #cluster-id: 1

  # Interface to be used when running Cilium on top of a CNI plugin.
  # For flannel, use "cni0"
  flannel-master-device: ""
  # When running Cilium with policy enforcement enabled on top of a CNI plugin
  # the BPF programs will be installed on the network interface specified in
  # 'flannel-master-device' and on all network interfaces belonging to
  # a container. When the Cilium DaemonSet is removed, the BPF programs will
  # be kept in the interfaces unless this option is set to "true".
  flannel-uninstall-on-exit: "false"
  # Installs a BPF program to allow for policy enforcement in already running
  # containers managed by Flannel.
  # NOTE: This requires Cilium DaemonSet to be running in the hostPID.
  # To run in this mode in Kubernetes change the value of the hostPID from
  # false to true. Can be found under the path `spec.spec.hostPID`
  flannel-manage-existing-containers: "false"

  # DNS Polling periodically issues a DNS lookup for each `matchName` from
  # cilium-agent. The result is used to regenerate endpoint policy.
  # DNS lookups are repeated with an interval of 5 seconds, and are made for
  # A(IPv4) and AAAA(IPv6) addresses. Should a lookup fail, the most recent IP
  # data is used instead. An IP change will trigger a regeneration of the Cilium
  # policy for each endpoint and increment the per cilium-agent policy
  # repository revision.
  #
  # This option is disabled by default starting from version 1.4.x in favor
  # of a more powerful DNS proxy-based implementation, see [0] for details.
  # Enable this option if you want to use FQDN policies but do not want to use
  # the DNS proxy.
  #
  # To ease upgrade, users may opt to set this option to "true".
  # Otherwise please refer to the Upgrade Guide [1] which explains how to
  # prepare policy rules for upgrade.
  #
  # [0] http://docs.cilium.io/en/stable/policy/language/#dns-based
  # [1] http://docs.cilium.io/en/stable/install/upgrade/#changes-that-may-require-action
  tofqdns-enable-poller: "false"

  # wait-bpf-mount makes init container wait until bpf filesystem is mounted
  wait-bpf-mount: "true"

  # Enable legacy services (prior v1.5) to prevent from terminating existing
  # connections with services when upgrading Cilium from < v1.5 to v1.5.
  enable-legacy-services: "false"

---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
  labels:
    io.cilium/app: operator
    name: cilium-operator
  name: cilium-operator
  namespace: kube-system
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 0
  selector:
    matchLabels:
      rubix-app: cilium-operator
  template:
    metadata:
      labels:
        rubix-app: cilium-operator
        com.palantir.sls.status/sls-status: ""
      annotations:
        ad.datadoghq.com/cilium-operator.check_names: |
          [
            "openmetrics"
          ]
        ad.datadoghq.com/cilium-operator.init_configs: |
          [{}]
        ad.datadoghq.com/cilium-operator.instances: |
          [
            {
              "prometheus_url": "http://localhost:{{ $metricsPort }}/metrics",
              "namespace": "cilium",
              "tags": [
                "product:cilium-operator"
              ],
              "metrics": [
                "*"
              ],
              "send_histograms_buckets": false,
              "send_distribution_buckets": false
            }
          ]
        com.palantir.k8sapplications/hostports: |
          {
            "host-ports": [
              {{ $healthzPort }},
              {{ $metricsPort }},
              {{ $healthCheckSidecarPort }},
              {{ $slsStatusReporterAppPort }},
              {{ $slsStatusReporterMgmtPort }}
            ]
          }
        com.palantir.rubix.pod/creds-for-cloud-role: cilium-operator-v0
        com.palantir.rubix.pod/pod-key-pair-v2: "{}"
        com.palantir.rubix.pod/sls-service-info-v2: >
          {
            "service-name": "cilium-operator",
            "service-id": "cilium-operator",
            "stack-name": "{{ $kubeSystemNamespace }}",
            "stack-id": "{{ $kubeSystemNamespace }}",
            "containers": {
              "cilium-operator": {
                "product-name": "cilium-operator",
                "product-version": "{{ .ApplicationVersion }}"
              },
              "health-check-sidecar": {
                "product-name": "health-check-sidecar",
                "product-version": "0.4.0"
              }
            }
          }
        com.palantir.deployability.config-content/startup.yml: |
          server:
            endpoint-type: https
            client-ca-files:
            cert-file: /mnt/secrets/certs/cert.pem
            key-file: /mnt/secrets/certs/key.pem
            address: 0.0.0.0
            port: {{ $healthCheckSidecarPort }}
            management-port: {{ $healthCheckSidecarPort }}
          clients:
            services:
              cilium-operator:
                uris:
                - "http://0.0.0.0:{{ $healthzPort }}/healthz"
          health-checks:
          - type: http
            service: cilium-operator
        com.palantir.deployability.config-content/runtime.yml: |
          logging:
            level: debug
            output: STDOUT
        com.palantir.deployability/status-sidecar-ports: |
          {
            "application": {{ $slsStatusReporterAppPort }},
            "management": {{ $slsStatusReporterMgmtPort }}
          }
        com.palantir.sls.status/metadata: |
          {
            "entityId": "cilium-operator",
            "entityName": "cilium-operator",
            "productName": "operator",
            "productGroup": "com.palantir.deployability.cilium",
            "productVersion": "{{ .ApplicationVersion }}",
            "stackName": "{{ $kubeSystemNamespace }}",
            "stackId": "{{ $kubeSystemNamespace }}"
          }
        com.palantir.sls.status/probe: |
          {
            "containerProbes": {
              "health-check-sidecar": {
                "path": "/",
                "port": "{{ $healthCheckSidecarPort }}"
              }
            }
          }
    spec:
      serviceAccount: cilium-operator
      tolerations:
        - key: dedicated
          value: control-plane
          operator: Equal
          effect: NoSchedule
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
        - effect: NoSchedule
          key: node.kubernetes.io/not-ready
          operator: Exists
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: nodetype
                    operator: In
                    values:
                      - control-plane
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: rubix-app
                    operator: In
                    values:
                      - cilium-operator
              topologyKey: "kubernetes.io/hostname"
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: rubix-app
                      operator: In
                      values:
                        - cilium-operator
                topologyKey: "failure-domain.beta.kubernetes.io/zone"
      dnsPolicy: ClusterFirstWithHostNet
      hostNetwork: true
      terminationGracePeriodSeconds: 30
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      volumes:
        - configMap:
            defaultMode: 420
            items:
              - key: etcd-config
                path: etcd.config
            name: cilium-config
          name: etcd-config-path
        - name: health-check-config
          downwardAPI:
            items:
              - path: "startup.yml"
                fieldRef:
                  fieldPath: metadata.annotations['com.palantir.deployability.config-content/startup.yml']
              - path: "runtime.yml"
                fieldRef:
                  fieldPath: metadata.annotations['com.palantir.deployability.config-content/runtime.yml']
      containers:
        - name: cilium-operator
          command: ["/usr/local/bin/planer"]
          args:
          - "--log-output=stdout"
          - "--line-buffer-size=1310720"
          - "--filters=/etc/planer/operator.planer"
          - "--"
          - "/usr/bin/cilium-operator"
          - "--debug=$(CILIUM_DEBUG)"
          - "--kvstore=etcd"
          - "--kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config"
          - "--ipam=eni"
          - "--enable-metrics=true"
          - "--metrics-address=:{{ $metricsPort }}"
          - "--aws-client-qps=100"
          - "--aws-client-burst=500"
          - "--k8s-client-qps=50"
          - "--k8s-client-burst=100"
          - "--cnp-node-status-gc=false"
          - "--identity-gc-interval=5m"
          - "--enable-k8s-endpoint-slice=false"
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: CILIUM_DEBUG
              valueFrom:
                configMapKeyRef:
                  key: debug
                  name: cilium-config
                  optional: true
            - name: CILIUM_CLUSTER_NAME
              valueFrom:
                configMapKeyRef:
                  key: cluster-name
                  name: cilium-config
                  optional: true
            - name: CILIUM_CLUSTER_ID
              valueFrom:
                configMapKeyRef:
                  key: cluster-id
                  name: cilium-config
                  optional: true
            - name: CILIUM_DISABLE_ENDPOINT_CRD
              valueFrom:
                configMapKeyRef:
                  key: disable-endpoint-crd
                  name: cilium-config
                  optional: true
          image: "docker.external.palantir.build/deployability/cilium/operator:{{ .ApplicationVersion }}"
          imagePullPolicy: Always
          livenessProbe:
            httpGet:
              host: '127.0.0.1'
              path: /healthz
              port: {{ $healthzPort }}
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 3
          volumeMounts:
          - mountPath: /var/lib/etcd-config
            name: etcd-config-path
            readOnly: true
          resources:
            requests:
              cpu: {{ .Variables.CiliumOperatorCPURequests }}
            limits:
              cpu: 3
              memory: {{ .Variables.CiliumOperatorMemoryLimits }}
        - name: health-check-sidecar
          image: "docker.external.palantir.build/deployability/health-check-sidecar:0.4.0"
          args:
          - server
          - --startup-config
          - /etc/health-check-sidecar/startup.yml
          - --runtime-config
          - /etc/health-check-sidecar/runtime.yml
          resources:
            limits:
              cpu: {{ .Variables.HealthCheckSidecarCPULimits }}
              memory: {{ .Variables.HealthCheckSidecarMemoryLimits }}
          volumeMounts:
            - name: health-check-config
              mountPath: "/etc/health-check-sidecar"
          readinessProbe:
            httpGet:
              path: /status/readiness
              port: {{ $healthCheckSidecarPort }}
              scheme: HTTPS
          livenessProbe:
            httpGet:
              path: /status/liveness
              port: {{ $healthCheckSidecarPort }}
              scheme: HTTPS
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cilium-operator
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cilium-operator
rules:
  - apiGroups:
      - ""
    resources:
      # to get k8s version and status
      - componentstatuses
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      # to automatically delete [core|kube]dns pods so that are starting to being
      # managed by Cilium
      - pods
    verbs:
      - get
      - list
      - watch
      - delete
  - apiGroups:
      - ""
    resources:
      # to automatically read from k8s and import the node's pod CIDR to cilium's
      # etcd so all nodes know how to reach another pod running in in a different
      # node.
      - nodes
      # to perform the translation of a CNP that contains `ToGroup` to its endpoints
      - services
      - endpoints
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumnetworkpolicies
      - ciliumnetworkpolicies/status
      - ciliumclusterwidenetworkpolicies
      - ciliumclusterwidenetworkpolicies/status
      - ciliumendpoints
      - ciliumendpoints/status
      - ciliumnodes
      - ciliumnodes/status
      - ciliumidentities
      - ciliumidentities/status
    verbs:
      - '*'
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cilium-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cilium-operator
subjects:
  - kind: ServiceAccount
    name: cilium-operator
    namespace: kube-system
